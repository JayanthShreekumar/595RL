{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multiple_tensorboard_data(log_dir, tags_to_save, output_dir):\n",
    "    ea = event_accumulator.EventAccumulator(log_dir)\n",
    "    ea.Reload()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for tag, title in tags_to_save:\n",
    "        try:\n",
    "            steps = []\n",
    "            values = []\n",
    "            for event in ea.Scalars(tag):\n",
    "                steps.append(event.step)\n",
    "                values.append(event.value)\n",
    "            \n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(steps, values)\n",
    "            plt.title(title)\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.savefig(f'{output_dir}/{title}.png')\n",
    "            plt.close()\n",
    "            print(f\"Saved plot for {title}\")\n",
    "        except KeyError:\n",
    "            print(f\"Tag {tag} not found in the logs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag {tag}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = [\n",
    "    (\"ray/tune/episode_reward_mean\", \"Episode Reward Mean\"),\n",
    "    (\"ray/tune/info/learner/adversary_0/learner_stats/actor_loss\", \"Adversary 0 Actor Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_0/learner_stats/critic_loss\", \"Adversary 0 Critic Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_0/learner_stats/mean_q\", \"Adversary 0 Mean Q\"),\n",
    "    (\"ray/tune/info/learner/adversary_1/learner_stats/actor_loss\", \"Adversary 1 Actor Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_1/learner_stats/critic_loss\", \"Adversary 1 Critic Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_1/learner_stats/mean_q\", \"Adversary 1 Mean Q\"),\n",
    "    (\"ray/tune/info/learner/adversary_2/learner_stats/actor_loss\", \"Adversary 2 Actor Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_2/learner_stats/critic_loss\", \"Adversary 2 Critic Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_2/learner_stats/mean_q\", \"Adversary 2 Mean Q\"),\n",
    "    (\"ray/tune/info/learner/adversary_3/learner_stats/actor_loss\", \"Adversary 3 Actor Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_3/learner_stats/critic_loss\", \"Adversary 3 Critic Loss\"),\n",
    "    (\"ray/tune/info/learner/adversary_3/learner_stats/mean_q\", \"Adversary 3 Mean Q\"),\n",
    "    (\"ray/tune/info/learner/agent_0/learner_stats/actor_loss\", \"Agent 0 Actor Loss\"),\n",
    "    (\"ray/tune/info/learner/agent_0/learner_stats/critic_loss\", \"Agent 0 Critic Loss\"),\n",
    "    (\"ray/tune/info/learner/agent_0/learner_stats/mean_q\", \"Agent 0 Mean Q\"),\n",
    "    (\"ray/tune/info/learner/agent_1/learner_stats/actor_loss\", \"Agent 1 Actor Loss\"),\n",
    "    (\"ray/tune/info/learner/agent_1/learner_stats/critic_loss\", \"Agent 1 Critic Loss\"),\n",
    "    (\"ray/tune/info/learner/agent_1/learner_stats/mean_q\", \"Agent 1 Mean Q\"),\n",
    "    (\"ray/tune/policy_reward_mean/adversary_0\", \"Adversary 0 Policy Reward Mean\"),\n",
    "    (\"ray/tune/policy_reward_mean/adversary_1\", \"Adversary 1 Policy Reward Mean\"),\n",
    "    (\"ray/tune/policy_reward_mean/adversary_2\", \"Adversary 2 Policy Reward Mean\"),\n",
    "    (\"ray/tune/policy_reward_mean/adversary_3\", \"Adversary 3 Policy Reward Mean\"),\n",
    "    (\"ray/tune/policy_reward_mean/agent_0\", \"Agent 0 Policy Reward Mean\"),\n",
    "    (\"ray/tune/policy_reward_mean/agent_1\", \"Agent 1 Policy Reward Mean\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot for Episode Reward Mean\n",
      "Saved plot for Adversary 0 Actor Loss\n",
      "Saved plot for Adversary 0 Critic Loss\n",
      "Saved plot for Adversary 0 Mean Q\n",
      "Saved plot for Adversary 1 Actor Loss\n",
      "Saved plot for Adversary 1 Critic Loss\n",
      "Saved plot for Adversary 1 Mean Q\n",
      "Saved plot for Adversary 2 Actor Loss\n",
      "Saved plot for Adversary 2 Critic Loss\n",
      "Saved plot for Adversary 2 Mean Q\n",
      "Saved plot for Adversary 3 Actor Loss\n",
      "Saved plot for Adversary 3 Critic Loss\n",
      "Saved plot for Adversary 3 Mean Q\n",
      "Saved plot for Agent 0 Actor Loss\n",
      "Saved plot for Agent 0 Critic Loss\n",
      "Saved plot for Agent 0 Mean Q\n",
      "Saved plot for Agent 1 Actor Loss\n",
      "Saved plot for Agent 1 Critic Loss\n",
      "Saved plot for Agent 1 Mean Q\n",
      "Saved plot for Adversary 0 Policy Reward Mean\n",
      "Saved plot for Adversary 1 Policy Reward Mean\n",
      "Saved plot for Adversary 2 Policy Reward Mean\n",
      "Saved plot for Adversary 3 Policy Reward Mean\n",
      "Saved plot for Agent 0 Policy Reward Mean\n",
      "Saved plot for Agent 1 Policy Reward Mean\n"
     ]
    }
   ],
   "source": [
    "plot_multiple_tensorboard_data('./SAC_independent_policy_both\\SAC_2024-11-27_14-37-06/SAC_simple_tag_v3_fbf6d_00000_0_2024-11-27_14-37-06',\\\n",
    "                      to_save, \"./plots/SAC_independent_policy_both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot for Episode Reward Mean\n",
      "Saved plot for Adversary 0 Actor Loss\n",
      "Saved plot for Adversary 0 Critic Loss\n",
      "Saved plot for Adversary 0 Mean Q\n",
      "Saved plot for Adversary 1 Actor Loss\n",
      "Saved plot for Adversary 1 Critic Loss\n",
      "Saved plot for Adversary 1 Mean Q\n",
      "Saved plot for Adversary 2 Actor Loss\n",
      "Saved plot for Adversary 2 Critic Loss\n",
      "Saved plot for Adversary 2 Mean Q\n",
      "Saved plot for Adversary 3 Actor Loss\n",
      "Saved plot for Adversary 3 Critic Loss\n",
      "Saved plot for Adversary 3 Mean Q\n",
      "Tag ray/tune/info/learner/agent_0/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_0/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_0/learner_stats/mean_q not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/mean_q not found in the logs\n",
      "Saved plot for Adversary 0 Policy Reward Mean\n",
      "Saved plot for Adversary 1 Policy Reward Mean\n",
      "Saved plot for Adversary 2 Policy Reward Mean\n",
      "Saved plot for Adversary 3 Policy Reward Mean\n",
      "Saved plot for Agent 0 Policy Reward Mean\n",
      "Saved plot for Agent 1 Policy Reward Mean\n"
     ]
    }
   ],
   "source": [
    "plot_multiple_tensorboard_data('./SAC_independent_policy_red_only/SAC_2024-11-29_00-39-05/SAC_simple_tag_v3_3f732_00000_0_2024-11-29_00-39-06/',\\\n",
    "                      to_save, \"./plots/SAC_independent_policy_red_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot for Episode Reward Mean\n",
      "Saved plot for Adversary 0 Actor Loss\n",
      "Saved plot for Adversary 0 Critic Loss\n",
      "Saved plot for Adversary 0 Mean Q\n",
      "Tag ray/tune/info/learner/adversary_1/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_1/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_1/learner_stats/mean_q not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_2/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_2/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_2/learner_stats/mean_q not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_3/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_3/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_3/learner_stats/mean_q not found in the logs\n",
      "Saved plot for Agent 0 Actor Loss\n",
      "Saved plot for Agent 0 Critic Loss\n",
      "Saved plot for Agent 0 Mean Q\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/mean_q not found in the logs\n",
      "Saved plot for Adversary 0 Policy Reward Mean\n",
      "Tag ray/tune/policy_reward_mean/adversary_1 not found in the logs\n",
      "Tag ray/tune/policy_reward_mean/adversary_2 not found in the logs\n",
      "Tag ray/tune/policy_reward_mean/adversary_3 not found in the logs\n",
      "Saved plot for Agent 0 Policy Reward Mean\n",
      "Tag ray/tune/policy_reward_mean/agent_1 not found in the logs\n"
     ]
    }
   ],
   "source": [
    "plot_multiple_tensorboard_data(\"./SAC_single_policy_both/SAC_2024-12-01_17-43-28/SAC_simple_tag_v3_aec77_00000_0_2024-12-01_17-43-28/\",\\\n",
    "                      to_save, \"./plots/SAC_single_policy_both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved plot for Episode Reward Mean\n",
      "Saved plot for Adversary 0 Actor Loss\n",
      "Saved plot for Adversary 0 Critic Loss\n",
      "Saved plot for Adversary 0 Mean Q\n",
      "Tag ray/tune/info/learner/adversary_1/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_1/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_1/learner_stats/mean_q not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_2/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_2/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_2/learner_stats/mean_q not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_3/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_3/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/adversary_3/learner_stats/mean_q not found in the logs\n",
      "Tag ray/tune/info/learner/agent_0/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_0/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_0/learner_stats/mean_q not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/actor_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/critic_loss not found in the logs\n",
      "Tag ray/tune/info/learner/agent_1/learner_stats/mean_q not found in the logs\n",
      "Saved plot for Adversary 0 Policy Reward Mean\n",
      "Tag ray/tune/policy_reward_mean/adversary_1 not found in the logs\n",
      "Tag ray/tune/policy_reward_mean/adversary_2 not found in the logs\n",
      "Tag ray/tune/policy_reward_mean/adversary_3 not found in the logs\n",
      "Saved plot for Agent 0 Policy Reward Mean\n",
      "Tag ray/tune/policy_reward_mean/agent_1 not found in the logs\n"
     ]
    }
   ],
   "source": [
    "plot_multiple_tensorboard_data(\"./SAC_single_policy_red_only/SAC_2024-12-01_10-59-31/SAC_simple_tag_v3_406af_00000_0_2024-12-01_10-59-31/\",\\\n",
    "                      to_save, \"./plots/SAC_single_policy_red_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorboard.backend.event_processing import event_accumulator\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = [\n",
    "    (\"ray/tune/episode_reward_mean\", \"Episode Reward Mean\", \"Individual Policies\",\"Single Policy\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlaid_plots(log_dir1, log_dir2, tags_to_save, output_dir):\n",
    "    ea1 = event_accumulator.EventAccumulator(log_dir1)\n",
    "    ea1.Reload()\n",
    "    ea2 = event_accumulator.EventAccumulator(log_dir2)\n",
    "    ea2.Reload()\n",
    "    \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for tag, title, label1, label2 in tags_to_save:\n",
    "        try:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            \n",
    "            # Plot data from log_dir1\n",
    "            steps1, values1 = [], []\n",
    "            for event in ea1.Scalars(tag):\n",
    "                steps1.append(event.step)\n",
    "                values1.append(event.value)\n",
    "            plt.plot(steps1, values1, label=label1)\n",
    "            \n",
    "            # Plot data from log_dir2\n",
    "            steps2, values2 = [], []\n",
    "            for event in ea2.Scalars(tag):\n",
    "                steps2.append(event.step)\n",
    "                values2.append(event.value)\n",
    "            plt.plot(steps2, values2, label=label2)\n",
    "            \n",
    "            plt.title(title)\n",
    "            plt.xlabel('Step')\n",
    "            plt.ylabel('Value')\n",
    "            plt.legend()\n",
    "            plt.savefig(f'{output_dir}/{title}.png')\n",
    "            plt.close()\n",
    "            print(f\"Saved overlaid plot for {title}\")\n",
    "        except KeyError:\n",
    "            print(f\"Tag {tag} not found in one or both logs\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing tag {tag}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved overlaid plot for Episode Reward Mean\n"
     ]
    }
   ],
   "source": [
    "overlaid_plots('./SAC_independent_policy_both\\SAC_2024-11-27_14-37-06/SAC_simple_tag_v3_fbf6d_00000_0_2024-11-27_14-37-06',\\\n",
    "               \"./SAC_single_policy_both/SAC_2024-12-01_17-43-28/SAC_simple_tag_v3_aec77_00000_0_2024-12-01_17-43-28/\",\n",
    "                      to_save, \"./plots/multiple_vs_single_policy_both\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = [\n",
    "    (\"ray/tune/episode_reward_mean\", \"Episode Reward Mean, multiple policies\", \"Both trained\",\"Only agents trained\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved overlaid plot for Episode Reward Mean, multiple policies\n"
     ]
    }
   ],
   "source": [
    "overlaid_plots('./SAC_independent_policy_both\\SAC_2024-11-27_14-37-06/SAC_simple_tag_v3_fbf6d_00000_0_2024-11-27_14-37-06',\\\n",
    "               './SAC_independent_policy_red_only/SAC_2024-11-29_00-39-05/SAC_simple_tag_v3_3f732_00000_0_2024-11-29_00-39-06/',\n",
    "                      to_save, \"./plots/single_both_vs_agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_save = [\n",
    "    (\"ray/tune/episode_reward_mean\", \"Episode Reward Mean, single policy\", \"Both trained\",\"Only agents trained\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved overlaid plot for Episode Reward Mean, single policy\n"
     ]
    }
   ],
   "source": [
    "overlaid_plots(\"./SAC_single_policy_both/SAC_2024-12-01_17-43-28/SAC_simple_tag_v3_aec77_00000_0_2024-12-01_17-43-28/\",\n",
    "               \"./SAC_single_policy_red_only/SAC_2024-12-01_10-59-31/SAC_simple_tag_v3_406af_00000_0_2024-12-01_10-59-31/\",\n",
    "                      to_save, \"./plots/multiple_both_vs_agents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
