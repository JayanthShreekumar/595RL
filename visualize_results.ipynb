{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy\n",
    "import torch\n",
    "import sklearn\n",
    "import supersuit as ss\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from ray import tune\n",
    "from ray.rllib.algorithms.ddpg import DDPG, DDPGConfig\n",
    "from ray.rllib.algorithms.sac import SAC, SACConfig\n",
    "from ray.rllib.env.wrappers.pettingzoo_env import ParallelPettingZooEnv\n",
    "from pettingzoo.mpe import simple_tag_v3\n",
    "import gymnasium as gym\n",
    "\n",
    "checkpoint_path = r\"C:\\Users\\rkamatar\\Downloads\\checkpoint_000013\"\n",
    "video_filename='sac_policy_11_27_T14_37_checkpoint13.mp4'\n",
    "\n",
    "def env_creator():\n",
    "    env = simple_tag_v3.parallel_env(num_good=2, num_adversaries=4, num_obstacles=0, max_cycles=100, continuous_actions=True, render_mode=\"rgb_array\")\n",
    "    env = ss.pad_observations_v0(env)\n",
    "    env = ss.pad_action_space_v0(env)\n",
    "    env = ss.frame_stack_v1(env, 3)\n",
    "    env = ss.dtype_v0(env, np.float32)  # Ensure observations are float32\n",
    "    return env\n",
    "\n",
    "# env = env_creator()\n",
    "\n",
    "def load_checkpoint(checkpoint_path, config):\n",
    "    algo = SAC(config=config)\n",
    "    # algo.restore(checkpoint_path)\n",
    "    return algo\n",
    "\n",
    "def save_rendered_frame(frame, frame_count):\n",
    "    # Convert frame from RGB to BGR for OpenCV compatibility\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "    \n",
    "    # Save the frame as an image file\n",
    "    frame_file = f'{frame_dir}/frame_{frame_count}.png'\n",
    "    cv2.imwrite(frame_file, frame)\n",
    "\n",
    "def save_frame(obs, frame_count):\n",
    "    for agent, observation in obs.items():\n",
    "        # Ensure the observation is in the correct format (HxWxC)\n",
    "        if observation.ndim == 3 and observation.shape[0] == 3:  # If CxHxW format\n",
    "            frame = observation.transpose(1, 2, 0)\n",
    "        else:\n",
    "            frame = observation\n",
    "\n",
    "        # Normalize and convert to uint8 if necessary\n",
    "        if frame.dtype != np.uint8:\n",
    "            frame = (frame * 255).astype(np.uint8)\n",
    "\n",
    "        # Ensure the frame is in RGB format (OpenCV uses BGR)\n",
    "        if frame.shape[-1] == 3:\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        frame_file = f'{frame_dir}/frame_{frame_count}_{agent}.png'\n",
    "        cv2.imwrite(frame_file, frame)\n",
    "\n",
    "    return frame_count + 1\n",
    "\n",
    "def render_environment(env, algo, frame_dir, video_dir, num_episodes=5, video_filename=f\"output_video.mp4\", fps=10):\n",
    "    env.reset()\n",
    "    frame_count = 0\n",
    "    \n",
    "    # Initialize video writer\n",
    "    video_writer = None\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        obs, _ = env.reset()\n",
    "        done = {agent: False for agent in obs.keys()}\n",
    "        episode_reward = 0\n",
    "        step = 0\n",
    "        \n",
    "        while not all(done.values()):\n",
    "            actions = {agent: algo.compute_single_action(obs[agent], policy_id=agent) for agent in obs.keys()}\n",
    "            obs, rewards, terminated, truncated, _ = env.step(actions)\n",
    "            done = {agent: terminated[agent] or truncated[agent] for agent in obs.keys()}\n",
    "            episode_reward += sum(rewards.values())\n",
    "            \n",
    "            # Capture and save the rendered frame\n",
    "            rendered_frame = env.render()  # Get pixel array from render()\n",
    "            \n",
    "            if rendered_frame is not None:\n",
    "                # Save individual frames as images\n",
    "                save_rendered_frame(rendered_frame, frame_count)\n",
    "                \n",
    "                # Initialize VideoWriter if it's not already initialized\n",
    "                if video_writer is None:\n",
    "                    height, width, layers = rendered_frame.shape\n",
    "                    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # Codec for MP4\n",
    "                    video_writer = cv2.VideoWriter(f\"{video_dir}/{video_filename}\", fourcc, fps, (width, height))\n",
    "                \n",
    "                # Write the frame to the video file\n",
    "                video_writer.write(cv2.cvtColor(rendered_frame, cv2.COLOR_RGB2BGR))\n",
    "                \n",
    "                frame_count += 1\n",
    "            \n",
    "            time.sleep(0.1)\n",
    "            step += 1\n",
    "        \n",
    "        print(f\"Episode {episode + 1} finished after {step} steps. Total reward: {episode_reward}\")\n",
    "    \n",
    "    env.close()\n",
    "    \n",
    "    # Release the VideoWriter to finalize the video file\n",
    "    if video_writer is not None:\n",
    "        video_writer.release()\n",
    "    \n",
    "    print(f\"Frames saved in {frame_dir}\")\n",
    "    print(f\"Video saved as {video_filename}\")\n",
    "\n",
    "class CustomParallelPettingZooEnv(ParallelPettingZooEnv):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.env = env  # Store the original environment\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        return self.env.render()  # Pass the mode parameter\n",
    "\n",
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "env_name = \"simple_tag\"\n",
    "tune.register_env(env_name, lambda config: ParallelPettingZooEnv(env_creator()))\n",
    "\n",
    "# frame_dir = './outputs/saved_frames'\n",
    "# video_dir = \"./outputs/saved_video\"\n",
    "# if not os.path.exists(frame_dir):\n",
    "#     os.makedirs(frame_dir)\n",
    "#     os.makedirs(video_dir)\n",
    "config = (\n",
    "            SACConfig()\n",
    "            .environment(env='simple_tag')\n",
    "            .framework(\"torch\")\n",
    "            .rollouts(num_rollout_workers=7)\n",
    "            # .resources(num_cpus_per_worker=1, num_gpus_per_worker=1/8)\n",
    "            .training(\n",
    "                lr = 1e-4,\n",
    "                tau=.01,\n",
    "                train_batch_size=1024,\n",
    "                gamma=.95,\n",
    "            )\n",
    "            # .multi_agent(\n",
    "            #     policies={agent: (None, env.observation_space(agent), env.action_space(agent), {})\n",
    "            #             for agent in env.possible_agents},\n",
    "            #     policy_mapping_fn=lambda agent_id, *args, **kwargs: agent_id,\n",
    "            # )\n",
    "        )\n",
    "# Load the algorithm from the checkpoint\n",
    "# loaded_algo = load_checkpoint(checkpoint_path, config)\n",
    "algo = SAC(config=config)\n",
    "algo.restore(checkpoint_path)\n",
    "\n",
    "render_env = env_creator()\n",
    "print(algo.get_policy())\n",
    "policy = algo.get_policy()\n",
    "# Render the environment with the loaded algorithm\n",
    "frame_dir = './outputs/saved_frames'\n",
    "video_dir = \"./outputs/saved_video\"\n",
    "render_environment(render_env, algo, frame_dir, video_dir, num_episodes=2, video_filename=video_filename)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
